{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jupyter notebook for \"Data-Sitters Club Multilingual Mystery 2: Beware, Lee and Quinn!\"\n",
    "\n",
    "Published February 27, 2020\n",
    "\n",
    "v. 1.0\n",
    "\n",
    "Full text of this Data-Sitters Club book is available at [http://datasittersclub.github.io/site/dscm2](http://datasittersclub.github.io/site/dscm2).\n",
    "\n",
    "\n",
    "### Suggested citation:\n",
    "Dombrowski, Quinn. \"Jupyter notebook for *Data-Sitters Club Multilingual Mystery 2: Beware, Lee and Quinn!*\". February 27, 2020. https://github.com/datasittersclub/dscm2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quinn\n",
    "\n",
    "The differences leap out at you before you even open any of the books. \"**Christine** a une idée géniale\". Or was the Baby-Sitters Club \"L'idée géniale de **Valérie**:? Does \"**Bruno** aime **Mélanie**\", or is he instead \"Un amoureux pour **Anne-Marie**\"? There's a case study on the Dutch translation of the Baby-Sitters Club books in [Babysitting the reader: translating English narrative fiction for girls into Dutch (1946-1995](https://www.worldcat.org/title/babysitting-the-reader-translating-english-narrative-fiction-for-girls-into-dutch-1946-1995/oclc/230991531&referer=brief_results)) by Mieke K T Desmet that gets into strategies for localizing a story that takes place in a different cultural context, and the article \"[Cultural Understanding in the Indonesian Translation of The Baby-sitters Club](http://paradigma.ui.ac.id/index.php/paradigma/article/view/159)\" by Halida Aisyah talk about how the Indonesian translation took a different approach, maintaining the protagonists' foreign names and locations, and only adopting Indonesian cultural references when the American equivalent would've been incomprehensible without some kind of extensive explanation. But I hadn't come across any scholarly literature on translation strategies for The Baby-Sitters Club in French (in any of the translations: Québécois, Belgian, or French from France).\n",
    "\n",
    "\n",
    "I had questions, and not just \"what did they decide to call Mallory?\" (Spoiler alert: in Québécois it's Marjorie, and just like in English, it's the most frequently screwed up name when OCRing the books.) The ghostwriters in the US were working with an extensive \"BSC Bible\" that had the description and background of every character in Stoneybrook, and further afield in the BSC universe. (This was adapted and published as [The complete guide to The Baby-sitters Club](https://www.worldcat.org/title/complete-guide-to-the-baby-sitters-club/oclc/35328788&referer=brief_results).) But in [DSC Mystery #1: Lee and the Missing Metadata](https://datasittersclub.github.io/site/dscm1/), Lee discovered that at least in Quebec, they were throwing Baby-Sitters Club books at multiple translators, who turned them around in no time at all. How careful were the translators about consistency, in terms of what they called various peripheral characters and places? This was the making of another Data-Sitters Club Multilingual Mystery. (Who are the data-sitters? So glad you asked. Check out [Chapter 2](https://datasittersclub.github.io/site/chapter-2/).)\n",
    "\n",
    "\n",
    "Lee and I put our heads together about how we'd start looking into this mystery. We needed a book that we had on hand in all the translations: Québécois, Belgian, French from France (the last of these being the source of the recent French re-releases). We settled on Jessi's Secret Language, on the thought that all the major characters had been established by that point, as well as many peripheral ones. We'd need to compare with some of the other translations, but that would be our starting point.\n",
    "\n",
    "\n",
    "Here's the thing, though: Lee reads French. I don't. I mean, I could probably pick my way through the text and come up with a list of characters and places, but I had other ideas. I wanted to see how French named-entity recognition performed compared to English, when applied to The Baby-Sitters Club."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What’s named-entity recognition?\n",
    "Named-entity recognition (often abbreviated NER) is a kind of information extraction task -- basically, trying to identify particular things (like names of people, places, and organizations) in unstructured text, like a novel. (Yeah, I know that novels have structure, but your average plain-text file of a novel's text -- even if it maintains chapter headers and such -- doesn't have the kind of structure that a computer can easily read. I mean, it's not like it's a spreadsheet or something.) There are two major technical approaches: one uses grammar-based rules to identify the things of interest, and the other uses statistical models like machine learning, and requires a ton of labeled data (e.g. texts where a human has already gone through and correctly identified all the things of interest) upfront. Particularly for statistical models, the more your texts resemble the example texts that the model was trained on, the better the NER will perform. These models are most commonly trained on news corpora, or Wikipedia -- not 80's and 90's girls' literature. This sort of thing is a problem in DH more broadly, not just for us Data-Sitters. David Bamman's [LitBank project](https://github.com/dbamman/litbank) (a dataset of annotated excerpts from public domain literature) is one example of how DH scholars can significantly improve the effectiveness of natural-language processing (NLP) by training models on data that looks more like what we're trying to apply it to. But I'll save the question of how, exactly, one goes about training a model for a future Data-Sitters Club Multilingual Mystery. For the moment, let's see how some commonly-used tools perform \"out of the box\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The tools\n",
    "The two major NLP tools with multilingual coverage are [spaCy](https://spacy.io/) and [Stanford NLP](https://nlp.stanford.edu/software/). To use spaCy, you load it into Python and run it that way. While there's a Python version of Stanford NLP, as of February 2020 it doesn't cover everything -- and entities are one thing that's currently left out. To get entities with Stanford NLP, you have to run a memory-hungry Java program from the command line, with all the joy that comes from setting that up. To make matters worse, Stanford NLP doesn't have an NER model for French: just English, Spanish, German, and Chinese. It's a better comparison to look at English vs. French with the same tool, rather than English with one and French with the other, so for this mystery, we'll be using spaCy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The texts\n",
    "\n",
    "To make it easier to compare the entities from each text, I split up each translation plus the English original into 15 plain text files, one from every chapter. Everything else I left as I got it from ABBYY FineReader (as discussed in [DSC #2: Katia and the Phantom Corpus](https://datasittersclub.github.io/site/dsc2/)), plus the corrections to my (often bad) attempt to transcribe the \"handwritten text\" portions. I didn't appreciate some implications of that -- I'll get back to it in a bit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started with spaCy\n",
    "\n",
    "SpaCy is run via Python, so it can seem a little intimidating if you've never worked with a programming language before. For this mystery, I set up a Jupyter notebook in the Data-Sitters Club GitHub repo that you can download and use for your own texts. (If you're not familiar with Jupyter notebooks, [here's a Programming Historian tutorial](https://programminghistorian.org/en/lessons/jupyter-notebooks).)\n",
    "\n",
    "You can't run the exact same experiment I did without access to the same texts I have (which I can't share for copyright reasons), but the Jupyter notebook on GitHub has all the output I got running it on the Baby-Sitters Club corpus, so you can see the results of the process one step at a time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Downloading spaCy models\n",
    "\n",
    "The first step is to download the spaCy models. These models have been pre-trained on annotated French and English corpora, respectively. You only have to run these code cells below the first time you run the notebook; after that, you can skip right to step 2 and carry on from there. (If you run them again later, nothing bad will happen; it'll just download again.) You can also run spaCy in other notebooks on your computer in the future, and you’ll be able to skip the step of downloading the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports the module you need to download and install the spaCy French and English models\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting https://github.com/explosion/spacy-models/releases/download/fr_core_news_sm-2.2.0/fr_core_news_sm-2.2.0.tar.gz\n",
      "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/fr_core_news_sm-2.2.0/fr_core_news_sm-2.2.0.tar.gz (14.7MB)\n",
      "\u001b[K    100% |████████████████████████████████| 14.7MB 2.0MB/s ta 0:00:011   51% |████████████████▍               | 7.5MB 8.8MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied (use --upgrade to upgrade): fr-core-news-sm==2.2.0 from https://github.com/explosion/spacy-models/releases/download/fr_core_news_sm-2.2.0/fr_core_news_sm-2.2.0.tar.gz in ./anaconda3/lib/python3.6/site-packages\n",
      "Requirement already satisfied: spacy>=2.2.0 in ./anaconda3/lib/python3.6/site-packages (from fr-core-news-sm==2.2.0) (2.2.3)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in ./anaconda3/lib/python3.6/site-packages (from spacy>=2.2.0->fr-core-news-sm==2.2.0) (1.0.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in ./anaconda3/lib/python3.6/site-packages (from spacy>=2.2.0->fr-core-news-sm==2.2.0) (3.0.2)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in ./anaconda3/lib/python3.6/site-packages (from spacy>=2.2.0->fr-core-news-sm==2.2.0) (2.0.2)\n",
      "Requirement already satisfied: thinc<7.4.0,>=7.3.0 in ./anaconda3/lib/python3.6/site-packages (from spacy>=2.2.0->fr-core-news-sm==2.2.0) (7.3.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in ./anaconda3/lib/python3.6/site-packages (from spacy>=2.2.0->fr-core-news-sm==2.2.0) (1.16.2)\n",
      "Requirement already satisfied: setuptools in ./anaconda3/lib/python3.6/site-packages (from spacy>=2.2.0->fr-core-news-sm==2.2.0) (41.0.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in ./anaconda3/lib/python3.6/site-packages (from spacy>=2.2.0->fr-core-news-sm==2.2.0) (2.21.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in ./anaconda3/lib/python3.6/site-packages (from spacy>=2.2.0->fr-core-news-sm==2.2.0) (1.0.2)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in ./anaconda3/lib/python3.6/site-packages (from spacy>=2.2.0->fr-core-news-sm==2.2.0) (0.6.0)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in ./anaconda3/lib/python3.6/site-packages (from spacy>=2.2.0->fr-core-news-sm==2.2.0) (0.9.6)\n",
      "Requirement already satisfied: srsly<1.1.0,>=0.1.0 in ./anaconda3/lib/python3.6/site-packages (from spacy>=2.2.0->fr-core-news-sm==2.2.0) (1.0.1)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in ./anaconda3/lib/python3.6/site-packages (from spacy>=2.2.0->fr-core-news-sm==2.2.0) (0.4.1)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in ./anaconda3/lib/python3.6/site-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.0->fr-core-news-sm==2.2.0) (1.5.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in ./anaconda3/lib/python3.6/site-packages (from thinc<7.4.0,>=7.3.0->spacy>=2.2.0->fr-core-news-sm==2.2.0) (4.31.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./anaconda3/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.0->fr-core-news-sm==2.2.0) (2019.9.11)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in ./anaconda3/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.0->fr-core-news-sm==2.2.0) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in ./anaconda3/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.0->fr-core-news-sm==2.2.0) (2.8)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in ./anaconda3/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.0->fr-core-news-sm==2.2.0) (1.24.1)\n",
      "Requirement already satisfied: zipp>=0.5 in ./anaconda3/lib/python3.6/site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.0->fr-core-news-sm==2.2.0) (3.0.0)\n",
      "Building wheels for collected packages: fr-core-news-sm\n",
      "  Building wheel for fr-core-news-sm (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/qad/Library/Caches/pip/wheels/1e/5f/4c/b196e2768830b7636db9b6509af16e2bffc0da98b0725421dd\n",
      "Successfully built fr-core-news-sm\n"
     ]
    }
   ],
   "source": [
    "#Installs the French spaCy model\n",
    "!{sys.executable} -m pip install https://github.com/explosion/spacy-models/releases/download/fr_core_news_sm-2.2.0/fr_core_news_sm-2.2.0.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.0/en_core_web_sm-2.2.0.tar.gz\n",
      "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.0/en_core_web_sm-2.2.0.tar.gz (12.0MB)\n",
      "\u001b[K    100% |████████████████████████████████| 12.0MB 2.5MB/s ta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied (use --upgrade to upgrade): en-core-web-sm==2.2.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.0/en_core_web_sm-2.2.0.tar.gz in ./anaconda3/lib/python3.6/site-packages\n",
      "Requirement already satisfied: spacy>=2.2.0 in ./anaconda3/lib/python3.6/site-packages (from en-core-web-sm==2.2.0) (2.2.3)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in ./anaconda3/lib/python3.6/site-packages (from spacy>=2.2.0->en-core-web-sm==2.2.0) (3.0.2)\n",
      "Requirement already satisfied: thinc<7.4.0,>=7.3.0 in ./anaconda3/lib/python3.6/site-packages (from spacy>=2.2.0->en-core-web-sm==2.2.0) (7.3.1)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in ./anaconda3/lib/python3.6/site-packages (from spacy>=2.2.0->en-core-web-sm==2.2.0) (0.4.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in ./anaconda3/lib/python3.6/site-packages (from spacy>=2.2.0->en-core-web-sm==2.2.0) (1.0.2)\n",
      "Requirement already satisfied: numpy>=1.15.0 in ./anaconda3/lib/python3.6/site-packages (from spacy>=2.2.0->en-core-web-sm==2.2.0) (1.16.2)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in ./anaconda3/lib/python3.6/site-packages (from spacy>=2.2.0->en-core-web-sm==2.2.0) (1.0.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in ./anaconda3/lib/python3.6/site-packages (from spacy>=2.2.0->en-core-web-sm==2.2.0) (2.21.0)\n",
      "Requirement already satisfied: srsly<1.1.0,>=0.1.0 in ./anaconda3/lib/python3.6/site-packages (from spacy>=2.2.0->en-core-web-sm==2.2.0) (1.0.1)\n",
      "Requirement already satisfied: setuptools in ./anaconda3/lib/python3.6/site-packages (from spacy>=2.2.0->en-core-web-sm==2.2.0) (41.0.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in ./anaconda3/lib/python3.6/site-packages (from spacy>=2.2.0->en-core-web-sm==2.2.0) (2.0.2)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in ./anaconda3/lib/python3.6/site-packages (from spacy>=2.2.0->en-core-web-sm==2.2.0) (0.6.0)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in ./anaconda3/lib/python3.6/site-packages (from spacy>=2.2.0->en-core-web-sm==2.2.0) (0.9.6)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in ./anaconda3/lib/python3.6/site-packages (from thinc<7.4.0,>=7.3.0->spacy>=2.2.0->en-core-web-sm==2.2.0) (4.31.1)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in ./anaconda3/lib/python3.6/site-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.0->en-core-web-sm==2.2.0) (1.5.0)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in ./anaconda3/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.0->en-core-web-sm==2.2.0) (3.0.4)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in ./anaconda3/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.0->en-core-web-sm==2.2.0) (1.24.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./anaconda3/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.0->en-core-web-sm==2.2.0) (2019.9.11)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in ./anaconda3/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.0->en-core-web-sm==2.2.0) (2.8)\n",
      "Requirement already satisfied: zipp>=0.5 in ./anaconda3/lib/python3.6/site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.0->en-core-web-sm==2.2.0) (3.0.0)\n",
      "Building wheels for collected packages: en-core-web-sm\n",
      "  Building wheel for en-core-web-sm (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/qad/Library/Caches/pip/wheels/48/5c/1c/15f9d02afc8221a668d2172446dd8467b20cdb9aef80a172a4\n",
      "Successfully built en-core-web-sm\n"
     ]
    }
   ],
   "source": [
    "#Installs the English spaCy model\n",
    "!{sys.executable} -m pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.0/en_core_web_sm-2.2.0.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Importing spaCy and setting up NLP\n",
    "Run the code cell below to import the spaCy module, and create two functions: one which loads the French model and runs the NLP algorithms ( includes named-entity recognition), and one which does the same for the English. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports spaCy\n",
    "import spacy\n",
    "#Imports the French model\n",
    "import fr_core_news_sm\n",
    "#Sets up a function so you can run the French model on texts\n",
    "frnlp = fr_core_news_sm.load()\n",
    "#Imports the English model\n",
    "import en_core_web_sm\n",
    "#Sets up a function so you can run the English model on texts\n",
    "ennlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Importing other modules\n",
    "There’s various other modules that will be useful in this notebook. The code comments explain what each one is for. This code cell imports all of those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#io is used for opening and writing files\n",
    "import io\n",
    "#itertools is used for some of the iterative code\n",
    "#from itertools import chain\n",
    "#glob is used to find all the pathnames matching a specified pattern (here, all text files)\n",
    "import glob\n",
    "#os is used to navigate your folder directories (e.g. change folders to where you files are stored)\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Diretory setup\n",
    "Assuming you’re running Jupyter Notebook from your computer’s home directory, this code cell gives you the opportunity to change directories, into the directory where you’re keeping your French text files. (This notebook is designed to deal with one language at a time, and assumes your French text files are in one folder, and English are in another.)\n",
    "\n",
    "Replace `/Users/qad/Documents/dsc/dscm2` with the full path to the directory with your files.\n",
    "\n",
    "For instance, the default path to the Documents directory is (substituting your user name on the computer for YOUR-USER-NAME):\n",
    "\n",
    "- On Mac: '/Users/YOUR-USER-NAME/Documents'\n",
    "- On Windows: 'C:\\\\\\Users\\\\\\YOUR-USER-NAME\\\\\\Documents'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the file directory here\n",
    "filedirectory = '/Users/qad/Documents/dsc/dscm2'\n",
    "#Change the working directory to the one you just defined\n",
    "os.chdir(filedirectory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. French NER, first try\n",
    "\n",
    "The code cell in step 5 in the Jupyter notebook iterates through the files in the folder you specified up in step 4, after sorting them alphabetically. For every file that ends in .txt (an important limitation -- you'll get an error if you try to have Python open a file that isn't a text file, including those pesky invisible .DS_STORE files in just about every Mac folder), the code defines an output file name that involves appending '_ner_per.txt' to the end of the input filename.\n",
    "\n",
    "Opening the input file (i.e. each file in turn, one at a time) and the newly-created, empty output file, the code reads in the text of the input file, and runs the spaCy French NLP. Then, for every word recognized as an entity, as long as it's an entity labeled 'PER' (a person), the entity is written to the screen (with a print command) and to the output file. I thought it'd be easiest to work through the entities one type at a time, starting just with the character names.\\\n",
    "I wrote this code, a couple times pulling up previous notebooks I'd written that did similar things, and consulting the [spaCy documentation and examples](https://spacy.io/usage/linguistic-features#named-entities) for how to display the entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sort all the files in the directory you specified above, alphabetically.\n",
    "#For each of those files...\n",
    "for filename in sorted(os.listdir(filedirectory)):\n",
    "    #If the filename ends with .txt (i.e. if it's actually a text files)\n",
    "    if filename.endswith('.txt'):\n",
    "        #Write out below the name of the file\n",
    "        print(filename)\n",
    "        #The file name of the output file adds _ner_per to the end of the file name of the input file\n",
    "        outfilename = filename.replace('.txt', '_ner_per.txt')\n",
    "        #Open the infput filename\n",
    "        with open(filename, 'r') as f:\n",
    "            #Create and open the output filename\n",
    "            with open(outfilename, 'w') as out:\n",
    "                #Read the contents of the input file\n",
    "                chaptertext = f.read()\n",
    "                #Do French NLP on the contents of the input file\n",
    "                chapterner = frnlp(chaptertext)\n",
    "                #For each recognized entity\n",
    "                for ent in chapterner.ents:\n",
    "                    #If that entity is labeled as a person\n",
    "                    if ent.label_ == 'PER':\n",
    "                        #Print the entity, and the label (which should be PER)\n",
    "                        print(ent.text, ent.label_)\n",
    "                        #Write the entity to the output file\n",
    "                        out.write(ent.text)\n",
    "                        #Write a newline character to the output file\n",
    "                        out.write('\\n')\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And it worked... mostly? It was super weird that *J'ai* kept getting listed, but I wasn't too worried. A quirk of the model, plus the source text! Probably the model wasn't trained on first-person narratives like The Baby-Sitters Club. Yeah, there was also an example of *C'* that was harder to explain, but it wasn't until I saw an example of a double-curly-quote character (\") identified as an entity that I started getting suspicious. Could those be messing things up somehow?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Data cleaning\n",
    "Time for some data cleaning! When Lee brought the ABBYY FineReader output .txt files into Word to correct my bad transcriptions, Word \"helpfully\" replaced all the regular, straight single and double quotes with their curly equivalents. \n",
    "\n",
    "I wrote some code that opened every text file in my folder, searched for opening and closing curly quotes and replaced them with the “straight quote” character (a quotation mark that doesn’t differentiate opening and closing quotes). While I was at it, I saw that some of the texts weren’t using the straight single quote for the apostrophe, so I put that in there, too. This code overwrites the text files in the folder (rather than creating a new version) so if you want to keep your originals, make sure you have a copy elsewhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look for files in the source directory that end in .txt\n",
    "for filename in os.listdir(filedirectory):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        \n",
    "        #Open each file that ends in .txt\n",
    "        f = open(filename, 'r')\n",
    "        #Read the text\n",
    "        text = f.read()\n",
    "        #Replace curly double-quote with straight double-quote\n",
    "        lines = text.replace(\"“\", '\"')\n",
    "        lines = lines.replace('”', '\"')\n",
    "        #Replace curly singl-quote with straight single-quote\n",
    "        lines = lines.replace('’', \"'\")\n",
    "\n",
    "        #Write output to a new file with the same name as the original, overwriting the original file.\n",
    "        with open(filename, 'w') as out:\n",
    "            out.writelines(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. French NER, second try\n",
    "I didn’t make any changes to the code from step 5, but check out the difference in the results. Gone are those quotation marks as so-called entities -- along with all the examples of j’ai, c’, etc. All of those were showing up because they were using the curly single quote character, and that was messing up spaCy’s model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sort all the files in the directory you specified above, alphabetically.\n",
    "#For each of those files...\n",
    "for filename in sorted(os.listdir(filedirectory)):\n",
    "    #If the filename ends with .txt (i.e. if it's actually a text files)\n",
    "    if filename.endswith('.txt'):\n",
    "        #Write out below the name of the file\n",
    "        print(filename)\n",
    "        #The file name of the output file adds _ner_per to the end of the file name of the input file\n",
    "        outfilename = filename.replace('.txt', '_ner_per.txt')\n",
    "        #Open the infput filename\n",
    "        with open(filename, 'r') as f:\n",
    "            #Create and open the output filename\n",
    "            with open(outfilename, 'w') as out:\n",
    "                #Read the contents of the input file\n",
    "                chaptertext = f.read()\n",
    "                #Do French NLP on the contents of the input file\n",
    "                chapterner = frnlp(chaptertext)\n",
    "                #For each recognized entity\n",
    "                for ent in chapterner.ents:\n",
    "                    #If that entity is labeled as a person\n",
    "                    if ent.label_ == 'PER':\n",
    "                        #Print the entity, and the label (which should be PER)\n",
    "                        print(ent.text, ent.label_)\n",
    "                        #Write the entity to the output file\n",
    "                        out.write(ent.text)\n",
    "                        #Write a newline character to the output file\n",
    "                        out.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It’s not perfect: chapter 1 of the Quebec translation flags “mange Julie” (Julie eats) as a person instead of a name and noun. But it’s a lot better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. French NER for places\n",
    "\n",
    "I moved all the \\_ner_per.txt files into their own folder, so that spaCy wouldn’t try to run NER on text files of its own NER results, and would instead just use the files with the chapter texts as the objects of investigation.\n",
    "\n",
    "I changed the code from step 7 (and 5) to replace PER with LOC and ran it again to get location entities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sort all the files in the directory you specified above, alphabetically.\n",
    "#For each of those files...\n",
    "for filename in sorted(os.listdir(filedirectory)):\n",
    "    #If the filename ends with .txt (i.e. if it's actually a text files)\n",
    "    if filename.endswith('.txt'):\n",
    "        #Write out below the name of the file\n",
    "        print(filename)\n",
    "        #The file name of the output file adds _ner_lov to the end of the file name of the input file\n",
    "        outfilename = filename.replace('.txt', '_ner_loc.txt')\n",
    "        #Open the infput filename\n",
    "        with open(filename, 'r') as f:\n",
    "            #Create and open the output filename\n",
    "            with open(outfilename, 'w') as out:\n",
    "                #Read the contents of the input file\n",
    "                chaptertext = f.read()\n",
    "                #Do French NLP on the contents of the input file\n",
    "                chapterner = frnlp(chaptertext)\n",
    "                #For each recognized entity\n",
    "                for ent in chapterner.ents:\n",
    "                    #If that entity is labeled as a person\n",
    "                    if ent.label_ == 'LOC':\n",
    "                        #Print the entity, and the label (which should be PER)\n",
    "                        print(ent.text, ent.label_)\n",
    "                        #Write the entity to the output file\n",
    "                        out.write(ent.text)\n",
    "                        #Write a newline character to the output file\n",
    "                        out.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skimming the results, it was interesting to see how much worse it performed than the person entity recognition. It feels like a minority of the results are legit places, and most of the results are people’s names."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. French NER for orgs\n",
    "I was curious what I'd get by looking for entities flagged as organizations. I mean, this entire book series is about an organization: would that get flagged correctly? (Once again, I moved the \\_ner_loc.txt files into their own folder first.)\n",
    "\n",
    "The verdict: *Club des Baby* (France-French translation) and Club des baby (Quebec translation) get marked as organizations; the \"sitters\" gets lost when \"Baby-sitters\" gets separated at the hyphen. There's also various things that are most definitely not organizations that get tagged, like \"Bonjour!\" in Belgian ch. 10, or \"PLIÉ\" in Quebec ch. 3 (maybe spaCy thought it was an acronym and not a yelling dance teacher?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Sort all the files in the directory you specified above, alphabetically.\n",
    "#For each of those files...\n",
    "for filename in sorted(os.listdir(filedirectory)):\n",
    "    #If the filename ends with .txt (i.e. if it's actually a text files)\n",
    "    if filename.endswith('.txt'):\n",
    "        #Write out below the name of the file\n",
    "        print(filename)\n",
    "        #The file name of the output file adds _ner_org to the end of the file name of the input file\n",
    "        outfilename = filename.replace('.txt', '_ner_org.txt')\n",
    "        #Open the infput filename\n",
    "        with open(filename, 'r') as f:\n",
    "            #Create and open the output filename\n",
    "            with open(outfilename, 'w') as out:\n",
    "                #Read the contents of the input file\n",
    "                chaptertext = f.read()\n",
    "                #Do French NLP on the contents of the input file\n",
    "                chapterner = frnlp(chaptertext)\n",
    "                #For each recognized entity\n",
    "                for ent in chapterner.ents:\n",
    "                    #If that entity is labeled as a person\n",
    "                    if ent.label_ == 'ORG':\n",
    "                        #Print the entity, and the label (which should be PER)\n",
    "                        print(ent.text, ent.label_)\n",
    "                        #Write the entity to the output file\n",
    "                        out.write(ent.text)\n",
    "                        #Write a newline character to the output file\n",
    "                        out.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. French NER for misc\n",
    "The French entity model also has a “MISC” type, so for the sake of completeness, I couldn’t not try it. And the results are as advertised. Lots of names. Lots of “Ça”. There’s a “Tu es atroce” (You’re excruciating) from Ch. 5 of the France French version. “Le Langage Secret” in Ch. 6 of the Belgian translation gets flagged, and “P'tit” makes an appearance more than once. Only in the France French version do “Noirs” (Black people) and “Noire de mon école” (Black person in my school) get flagged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Sort all the files in the directory you specified above, alphabetically.\n",
    "#For each of those files...\n",
    "for filename in sorted(os.listdir(filedirectory)):\n",
    "    #If the filename ends with .txt (i.e. if it's actually a text files)\n",
    "    if filename.endswith('.txt'):\n",
    "        #Write out below the name of the file\n",
    "        print(filename)\n",
    "        #The file name of the output file adds _ner_misc to the end of the file name of the input file\n",
    "        outfilename = filename.replace('.txt', '_ner_misc.txt')\n",
    "        #Open the infput filename\n",
    "        with open(filename, 'r') as f:\n",
    "            #Create and open the output filename\n",
    "            with open(outfilename, 'w') as out:\n",
    "                #Read the contents of the input file\n",
    "                chaptertext = f.read()\n",
    "                #Do French NLP on the contents of the input file\n",
    "                chapterner = frnlp(chaptertext)\n",
    "                #For each recognized entity\n",
    "                for ent in chapterner.ents:\n",
    "                    #If that entity is labeled as a person\n",
    "                    if ent.label_ == 'MISC':\n",
    "                        #Print the entity, and the label (which should be PER)\n",
    "                        print(ent.text, ent.label_)\n",
    "                        #Write the entity to the output file\n",
    "                        out.write(ent.text)\n",
    "                        #Write a newline character to the output file\n",
    "                        out.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let’s do it all again for English\n",
    "The steps below in the notebook basically repeat the process described above, but using the spaCy model for English instead of French. At first, I just copied over the code cells from the French section, but kept getting some bizarre output: namely, it wasn’t able to find any entities labeled PER or LOC.\n",
    "\n",
    "To figure out what was going on, I removed the line `if ent.label_ == 'PER':` \n",
    "and un-indented the code nested inside it, to avoid Python indentation errors, then commented-out the following out.write lines by putting a # in front of them. I didn’t want to write any results, I just wanted to see what entities it found.\n",
    "\n",
    "Lo and behold, there were lots of entities, with more different entity labels than available for French. There’s GPE (geopolitical entity, AKA location, but not any of the locations that are like “so-and-so’s room”), DATE, CARDINAL (number type), LANGUAGE (seems relevant for Jessi’s Secret Language), TIME (e.g. “the morning of the day”), DATE (things like “a few weeks”, or, strangely, “eight-year-old”), PERSON (not PER)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endirectory = '/Users/qad/Documents/dsc/dscm2/en'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. English NER for people"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sort all the files in the directory you specified above, alphabetically.\n",
    "#For each of those files...\n",
    "for filename in sorted(os.listdir(filedirectory)):\n",
    "    #If the filename ends with .txt (i.e. if it's actually a text files)\n",
    "    if filename.endswith('.txt'):\n",
    "        #Write out below the name of the file\n",
    "        print(filename)\n",
    "        #The file name of the output file adds _ner_per to the end of the file name of the input file\n",
    "        outfilename = filename.replace('.txt', '_ner_per.txt')\n",
    "        #Open the infput filename\n",
    "        with open(filename, 'r') as f:\n",
    "            #Create and open the output filename\n",
    "            with open(outfilename, 'w') as out:\n",
    "                #Read the contents of the input file\n",
    "                chaptertext = f.read()\n",
    "                #Do English NLP on the contents of the input file\n",
    "                chapterner = ennlp(chaptertext)\n",
    "                #For each recognized entity\n",
    "                for ent in chapterner.ents:\n",
    "                    #If that entity is labeled as a person\n",
    "                    if ent.label_ == 'PERSON':\n",
    "                        #Print the entity, and the label (which should be PER)\n",
    "                        print(ent.text, ent.label_)\n",
    "                        #Write the entity to the output file\n",
    "                        out.write(ent.text)\n",
    "                        #Write a newline character to the output file\n",
    "                        out.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. English NER for places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sort all the files in the directory you specified above, alphabetically.\n",
    "#For each of those files...\n",
    "for filename in sorted(os.listdir(filedirectory)):\n",
    "    #If the filename ends with .txt (i.e. if it's actually a text files)\n",
    "    if filename.endswith('.txt'):\n",
    "        #Write out below the name of the file\n",
    "        print(filename)\n",
    "        #The file name of the output file adds _ner_loc to the end of the file name of the input file\n",
    "        outfilename = filename.replace('.txt', '_ner_loc.txt')\n",
    "        #Open the infput filename\n",
    "        with open(filename, 'r') as f:\n",
    "            #Create and open the output filename\n",
    "            with open(outfilename, 'w') as out:\n",
    "                #Read the contents of the input file\n",
    "                chaptertext = f.read()\n",
    "                #Do English NLP on the contents of the input file\n",
    "                chapterner = ennlp(chaptertext)\n",
    "                #For each recognized entity\n",
    "                for ent in chapterner.ents:\n",
    "                    #If that entity is labeled as a person\n",
    "                    if ent.label_ == 'GPE':\n",
    "                        #Print the entity, and the label (which should be PER)\n",
    "                        print(ent.text, ent.label_)\n",
    "                        #Write the entity to the output file\n",
    "                        out.write(ent.text)\n",
    "                        #Write a newline character to the output file\n",
    "                        out.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13. English NER for organizations\n",
    "I think my favorite entity type is ORG, which gets you everything from Oakley Elementary to Swanilda to Mama. (Yes, friends, “Daddy” is a PERSON but “Mama” is an ORG.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sort all the files in the directory you specified above, alphabetically.\n",
    "#For each of those files...\n",
    "for filename in sorted(os.listdir(filedirectory)):\n",
    "    #If the filename ends with .txt (i.e. if it's actually a text files)\n",
    "    if filename.endswith('.txt'):\n",
    "        #Write out below the name of the file\n",
    "        print(filename)\n",
    "        #The file name of the output file adds _ner_org to the end of the file name of the input file\n",
    "        outfilename = filename.replace('.txt', '_ner_org.txt')\n",
    "        #Open the infput filename\n",
    "        with open(filename, 'r') as f:\n",
    "            #Create and open the output filename\n",
    "            with open(outfilename, 'w') as out:\n",
    "                #Read the contents of the input file\n",
    "                chaptertext = f.read()\n",
    "                #Do English NLP on the contents of the input file\n",
    "                chapterner = ennlp(chaptertext)\n",
    "                #For each recognized entity\n",
    "                for ent in chapterner.ents:\n",
    "                    #If that entity is labeled as a person\n",
    "                    if ent.label_ == 'ORG':\n",
    "                        #Print the entity, and the label (which should be PER)\n",
    "                        print(ent.text, ent.label_)\n",
    "                        #Write the entity to the output file\n",
    "                        out.write(ent.text)\n",
    "                        #Write a newline character to the output file\n",
    "                        out.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14. English NER for works of art\n",
    "That said, there’s also a rare WORK_OF_ART entity type, exemplified by “Morning, Squirts”, “Hey, Jessi”, “On Top of Old Smoky” (depends on how you feel about folk music, I guess), and, my very favorite work of art, “Nope”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sort all the files in the directory you specified above, alphabetically.\n",
    "#For each of those files...\n",
    "for filename in sorted(os.listdir(filedirectory)):\n",
    "    #If the filename ends with .txt (i.e. if it's actually a text files)\n",
    "    if filename.endswith('.txt'):\n",
    "        #Write out below the name of the file\n",
    "        print(filename)\n",
    "        #The file name of the output file adds _ner_art to the end of the file name of the input file\n",
    "        outfilename = filename.replace('.txt', '_ner_art.txt')\n",
    "        #Open the infput filename\n",
    "        with open(filename, 'r') as f:\n",
    "            #Create and open the output filename\n",
    "            with open(outfilename, 'w') as out:\n",
    "                #Read the contents of the input file\n",
    "                chaptertext = f.read()\n",
    "                #Do English NLP on the contents of the input file\n",
    "                chapterner = ennlp(chaptertext)\n",
    "                #For each recognized entity\n",
    "                for ent in chapterner.ents:\n",
    "                    #If that entity is labeled as a person\n",
    "                    if ent.label_ == 'WORK_OF_ART':\n",
    "                        #Print the entity, and the label (which should be PER)\n",
    "                        print(ent.text, ent.label_)\n",
    "                        #Write the entity to the output file\n",
    "                        out.write(ent.text)\n",
    "                        #Write a newline character to the output file\n",
    "                        out.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What now?\n",
    "I reran the notebook for English, reluctantly limiting myself to the entity types held in common between the English and French models. So now I had, chapter-by-chapter, translation-by-translation (plus original English), all the person, location, and organization type entities.\n",
    "\n",
    "That’s nice.\n",
    "\n",
    "But that wasn’t my question: what I wanted to know was how the translators adapted people and place names. I had to figure out what to do with all these text files to get me closer to an answer.\n",
    "\n",
    "I had some thinking to do.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read more\n",
    "\n",
    "To find out how this *Data-Sitters Club Multilingual Mystery* ends, read the [full book on the Data-Sitters Club website](http://datasittersclub.github.io/site/dscm2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
